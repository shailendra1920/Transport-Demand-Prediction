{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Title: Public Transport Demand Prediction**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of a project on public transport demand prediction is to develop a system or model that can accurately forecast the number of passengers who will use a public transportation system over a given period of time. This requires collecting and analyzing historical data on passenger numbers and other relevant factors, as well as implementing machine learning algorithms and other techniques to make predictions.\n",
        "\n",
        "The project may involve developing a proof-of-concept prototype, a full-scale system for use by a public transportation provider, or a research study to evaluate the effectiveness of different prediction methods. Some of the key challenges in this type of project include dealing with data quality and missing data, accounting for changing patterns of demand over time, and selecting appropriate models and parameters to achieve accurate predictions.\n",
        "\n",
        "Overall, the aim of a project on public transport demand prediction is to help transportation providers optimize their services, reduce costs, and improve the overall passenger experience by anticipating and meeting the needs of their customers."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -** - https://github.com/ankurvish1920"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This challenge ask you to build a model that predicts the number of seats that Mobiticket can expect to sell in each ride, i.e. for a specific route on a specific date and time. There are 14 routes in this dataset. All of the routes ends in Nairobi and originate in towns to the North-West of Nairobi."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The towns from which these routes originate are:"
      ],
      "metadata": {
        "id": "Q3KaRCrrdmGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Awendo\n",
        "\n",
        "Homa Bay\n",
        "\n",
        "Kehancha\n",
        "\n",
        "Kendu Bay\n",
        "\n",
        "Keroka\n",
        "\n",
        "Keumbu\n",
        "\n",
        "Kijauri\n",
        "\n",
        "Kisii\n",
        "\n",
        "Mbita\n",
        "\n",
        "Migori\n",
        "\n",
        "Ndhiwa\n",
        "\n",
        "Nyachenge\n",
        "\n",
        "Oyugis\n",
        "\n",
        "Rodi\n",
        "\n",
        "Sirari\n",
        "\n",
        "Sori\n"
      ],
      "metadata": {
        "id": "FA9K2sHG6pEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### These routes from 14 origins at the first stop in the outskirts of Nairobi takes approximately 8 to 9 hours from the time of departure. From the first stop in the outskirts of Nairobi into the main bus terminal, where most passengers get off, in the Central Business District, takes another 2 to 3 hours depending on the traffic. The three stops that all these routes makes in Nairobi(in order) are:"
      ],
      "metadata": {
        "id": "InxvS67pdxzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1.   Kawangware: the first stop in the outskirts of Nairobi.\n",
        "2.   Westlands: \n",
        "3.   Afya centre: the bus centre where most passengers disembark."
      ],
      "metadata": {
        "id": "jkcu6BHBd5H6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###     Passengers from these bus (or shuttle ) rides are affected by the Nairobi traffic not only during there ride into the city, but from there they must continue their journey to there final destination in Nairobi wherever they may be. Traffic can act as a deterent for those who have the option to avoid buses that arrive in Nairobi during peak traffic hours. On the other hand traffic maybe an indication for people's movement patterns, reflection business hours, cultural events, political events, and holidays."
      ],
      "metadata": {
        "id": "7FxDH7V4d6eD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# Importing required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Importing Visualization Packages\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#importing skewwness data \n",
        "from scipy.stats import skew\n",
        "from scipy.stats import norm\n",
        "# Importing Modelling libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "# Importing warnings so as to handle the warnings. Python by default displays warnings once per module session. But, however sometimes we want to \n",
        "# ingore that, by importing these warnings you can control how warnings are handled and ensure that the code execute correctly.\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Importing rs2score to determine how well algorithms can predict target variable\n",
        "from sklearn.metrics import r2_score\n",
        "# Importing mean_squared_error to evaluate the distance between predicted and actual values of the variables.\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# importing MinMaxScaler to scale data between (0-1).\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Importing datetime to handle datetime of the data\n",
        "import datetime \n",
        "import time"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "##### **Here we are parsing the datetime columns in its proper format.**\n",
        "df= pd.read_csv('train_revised.csv',parse_dates=[\"travel_date\",\"travel_time\"])"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "# Dataset Duplicate Value Count\n",
        "len(df[df.duplicated()])\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset have a total 10 columns in which no columns have a null and dataset have no any duplicate values"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description "
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following are descriptions the fields.\n",
        "\n",
        "\n",
        "*   ride_id : unique ID of a vehicle on a specific route on a specific day and time.\n",
        "*   seat_number : seat assigned to the ticket.\n",
        "\n",
        "*   payment_method : Method used by customer to buy ticket from Mobiticket(Cash or Mpesa).\n",
        "*   payment_receipt : unique id number for ticket purchased from Mobiticket.\n",
        "\n",
        "*   travel_date : date of ride departure (MM/DD/YYYY).\n",
        "*   travel_time : Schedule departure time of ride, Rides generally depart on time. (hh:mm).\n",
        "*   travel_from : time from which ride originated.\n",
        "*   travel_to : designation of rides. All rides are to Nairobi.\n",
        "*   car_type : vehicle type (shuttle or bus).\n",
        "*   max_capacity : number of seats on a vehicle."
      ],
      "metadata": {
        "id": "zZP7dLeXLxCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# Check Unique Values for each variable.\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",df[i].nunique(),\".\")\n",
        "     "
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we want to prediction the number of tickets that Mobiticket can sell but in our data we are not having any dependent variable so we have to create it.\n",
        "# Here we are applying transform on a group and getting the count of non-nulls as storing these values into a new column called 'no_of_ticket'\n",
        "df['no_of_ticket']=df.groupby(['travel_date','travel_time','travel_from','car_type'])['travel_time'].transform(\"count\")\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping datetime is the process of converting dates and times into numerical values.\n",
        "df[\"day\"] = df['travel_date'].map(lambda x: x.day)\n",
        "df[\"year\"] = df['travel_date'].map(lambda x: x.year)\n",
        "df[\"month\"] = df['travel_date'].map(lambda x: x.month)"
      ],
      "metadata": {
        "id": "8mwHu90XkC05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a new columns name hour and mintus \n",
        "df['hour']= df['travel_time'].dt.hour\n",
        "\n",
        "df['minute'] =df['travel_time'].dt.minute"
      ],
      "metadata": {
        "id": "_yfdjjl4kFms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mapping week and weekdays into numerical values.\n",
        "df['week']= df['travel_date'].dt.week\n",
        "df['day_of_week']= df['travel_date'].dt.weekday\n",
        "df['day_of_week'] = df['travel_date'].dt.day_name()"
      ],
      "metadata": {
        "id": "pIy2fM7ikIjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dropping unnecessary columns.\n",
        "df=df.drop(['seat_number','ride_id','payment_receipt', 'travel_date','travel_time','travel_to'],axis= 1)"
      ],
      "metadata": {
        "id": "dKrO649ckK-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "BUj1VamMlTZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DIFFERENT TYPE OF CAR "
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#USING COUNTPLOT TO COUNT A NUMBER OF BUS AND SHUTTLE \n",
        "sns.countplot(x=df['car_type'])"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "countplot because it is a useful visualization tool for counting the frequency of each category in a categorical variable. In the context of booking data, a countplot can help us quickly understand how many bookings were made for each type of Car.\n",
        "\n",
        "The advantage of using a countplot over a simple bar chart is that the countplot automatically aggregates the data and counts the frequency of each category. This makes it easy to visualize the distribution of a categorical variable and compare the frequency of each category in a clear and concise way."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the number of bus booking is more as compared to shuttle booking."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "yes it will help transporation business to  analyse which car is most use."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "COUNTING WHICH PLACE IS MOST USE FOR TRANSPORATION "
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.countplot(x='travel_from', data=df, order=df['travel_from'].value_counts().index)"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "countplot because it is a useful visualization tool for counting the frequency of each category in a categorical variable. In the context of booking data, a countplot can help us quickly understand how many bookings were made for each type of booking.\n",
        "\n",
        "The advantage of using a countplot over a simple bar chart is that the countplot automatically aggregates the data and counts the frequency of each category. This makes it easy to visualize the distribution of a categorical variable and compare the frequency of each category in a clear and concise way."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kissi and migori are most busiest starting point of transporation and kendu bay are rarely uses"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WHICH DAY IS MOST BUSIEST"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "sns.countplot(x='day_of_week', data=df, order=df['day_of_week'].value_counts().index)"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A countplot is a type of plot in seaborn library that allows you to visualize the count of observations in a categorical variable. In the context of booking data, a countplot can be used to count the number of bookings for WEEK."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " the number of bookings for each day of the week in your dataset and found that Wednesday had the highest number of bookings, followed by Tuesday, Thursday, Friday, and Monday in that order"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact? \n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YES IT IS SHOWING WEDNESDAY AND TUESDAY ARE MOST BUSIEST DAY , COMPANY CAN USE THIS DATA TO MARKETING."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **WHICH YEAR HAVE MOST BOOKING**"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "df['year'].value_counts().plot(kind='pie', autopct='%1.1f%%')\n",
        "plt.title('Pie Chart of Column Name')\n",
        "plt.ylabel('')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO SEE A WHICH YEAR HAVE HIGHEST BOOKING PIECHART IS GOOD BECUASE IT IS SHOWING MORE VISUAL AND PRECENTAGE."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MOST BOOKING ARE IN 2018 AS COMPARED TO 2017"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WHICH HOUR IS MOST BUSIEST**"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.hist(df['hour'], bins=10)\n",
        "plt.title('busiest hour')\n",
        "plt.xlabel('hour Name')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HISTLPOT USES FOR VISUALISE  DISTRIBUTION OF DATA "
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MORNING TIME IS MOST BUSIEST HOUR AND WE CAN ALSO SAY MORE THAN 80% BOOKING ARE DONE IN MORNING."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MONTHLY BOOKING "
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.countplot(x='month', data=df, order=df['month'].value_counts().index)\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CORRELATION OF DATA "
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Chart - 7 visualization code\n",
        "plt.figure(figsize= (15,8))\n",
        "correlation= df.corr()\n",
        "sns.heatmap(abs(correlation),annot= True, cmap= 'coolwarm')"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overall, correlation is a useful tool for booking analysis data as it can help us understand the relationships between variables, build more accurate predictive models, and identify potential issues with multicollinearity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pair Plot "
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df)"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Our data contains lots of categorical data that we need to encode this into a numerical data. One such method to do this is labelencoding\n",
        "# LabelEncoder creates an ordering of the categorical values in the format that can be used by various algorithms.\n",
        "le= LabelEncoder()\n",
        "df['travel_from']= le.fit_transform(df['travel_from'])\n",
        "df['car_type']= le.fit_transform(df['car_type'])\n",
        "df['payment_method']= le.fit_transform(df['payment_method'])\n",
        "df['day_of_week']= le.fit_transform(df['day_of_week'])"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "dY60IyXI4F_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "fig, axs = plt.subplots(ncols=len(df.columns), figsize=(15,5))\n",
        "\n",
        "for i, col in enumerate(df.columns):\n",
        "    sns.boxplot(df[col], color='orange', ax=axs[i], width=0.3) \n",
        "    axs[i].set_title(col)\n",
        "\n",
        "plt.subplots_adjust(wspace=0.4)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hDI5SHIA71sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "boxplot showing me hour and mintus and travel_From  columns have high outliers."
      ],
      "metadata": {
        "id": "Evq20HWe9T6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df:\n",
        "#     print(df)\n",
        "    print(col  ,skew(df[col]))"
      ],
      "metadata": {
        "id": "ur12shll8Yi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "payment method , hour and mintus showing the high skewness , but we dont need payment method for machine learning model , we will delete this columns .but next syntax is for decrease skewness of hour and mintus columns ."
      ],
      "metadata": {
        "id": "Ugc4n2a2_Ivf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['hour']= np.log10(df['hour'])\n",
        "df['month']= np.sqrt(df['month'])\n",
        "df['car_type']= np.sqrt(df['car_type'])\n",
        "df['year']= np.sqrt(df['year'])"
      ],
      "metadata": {
        "id": "ZvFEUpv2EF4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df:\n",
        "#     print(df)\n",
        "    print(col  ,skew(df[col]))"
      ],
      "metadata": {
        "id": "tYirctOmFW_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over each column and create a displot\n",
        "for column in df.columns:\n",
        "    sns.displot(x=column, data=df, kde=True, rug=True)\n",
        "\n",
        "# Show the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l4ntcFjC9zHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#i dont need some columns for  machine learning.\n",
        "df.drop(columns=['payment_method','car_type','max_capacity','week','day_of_week'],axis=1,inplace=True)"
      ],
      "metadata": {
        "id": "k5RgwRBAMfEV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determing independent and dependent variable best suited for modelling \n",
        "# Data for all the independent variables\n",
        "## Importing required Libraries\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import stats\n",
        "from scipy.stats import zscore\n",
        "X = df.drop(labels='no_of_ticket',axis=1).apply(zscore)\n",
        "# Data for the dependent variable\n",
        "Y = df['no_of_ticket']"
      ],
      "metadata": {
        "id": "U3WUezZxB5IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.columns"
      ],
      "metadata": {
        "id": "W8XbU9QvMS33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA SPLITTING"
      ],
      "metadata": {
        "id": "2yOrcizr_3FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# Importing required Libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "#Divding the data set into Training and testing dataset using Test Train split\n",
        "#we have takes 80% - 20% ratio for Test Train Split\n",
        "X_train, X_test, Y_train, Y_test= train_test_split(X,Y, test_size= 0.2, random_state= 0)\n",
        "print(X_train.shape, Y_train.shape)\n",
        "print(X_test.shape, Y_test.shape)"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA SCALLING"
      ],
      "metadata": {
        "id": "E4xlP6_gAYVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## standardizing the values.\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler= StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "FoqdSK-gCnI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardization is another common data preprocessing technique used in machine learning to transform feature values to have zero mean and unit variance. In this technique, each feature is scaled so that it has a mean of 0 and a standard deviation of 1"
      ],
      "metadata": {
        "id": "sfyki0ASBrNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LINEAR REGRESSION MODEL"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "## importing linear regression\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "reg = LinearRegression().fit(X_train, Y_train)\n",
        "## fitting training data into linear regression model\n",
        "reg.score(X_train, Y_train)\n",
        "# Predict on the model\n",
        "# getting the test score of linear regrssion.\n",
        "print(f\"training_score {reg.score(X_train,Y_train)}\")\n",
        "print(f\"testing_score {reg.score(X_test,Y_test)}\")\n",
        "Y_actual=reg.predict(X_train)\n",
        "print(Y_actual)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred = reg.predict(X_test)\n",
        "Y_pred"
      ],
      "metadata": {
        "id": "4uXpJcy1PsYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg.coef_"
      ],
      "metadata": {
        "id": "7r9Sr7HGPyGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the the actual and predicted sales values\n",
        "plt.figure(figsize=(5,5))\n",
        "plt.plot(Y_pred[:50]**2)\n",
        "plt.plot(np.array((Y_test[:50])**2))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dkJrvmd2P2mL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## determining the distance of actual and the predicted output\n",
        "MSE= mean_squared_error(Y_test,Y_pred)\n",
        "print(\"MSE\", MSE)\n",
        "\n",
        "## taking root of the mean squared error.\n",
        "RMSE= np.sqrt(MSE)\n",
        "print(\"RMSE\", RMSE)"
      ],
      "metadata": {
        "id": "IyY44aQEPxG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# determing how our model is fitting the datapoint.\n",
        "# As the number of predictors in the model increases, the R-squared score tends to increase as well, even if the additional predictors do not contribute\n",
        "# significantly to the model's performance.\n",
        "from sklearn.metrics import r2_score\n",
        "r2_score= r2_score(10**Y_test,10**Y_pred)\n",
        "\n",
        "print(\"r2_score\", r2_score)"
      ],
      "metadata": {
        "id": "wc9rIlVmQF9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adjusted r2score iscore adjusts the R-squared score by penalizing the addition of unnecessary predictors.\n",
        "from sklearn.metrics import r2_score\n",
        "print(\"Adjusted R2 : \",1-(1-r2_score(10**(Y_test), 10**(Y_pred)))*((X_test.shape[0]-1)/(X_test.shape[0]-X_test.shape[1]-1)))"
      ],
      "metadata": {
        "id": "rhqfJOY8QLsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  XGBOOST REGRESSOR "
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## importing xgboost regressor\n",
        "from xgboost import XGBRegressor\n",
        "XG_model=XGBRegressor()\n",
        "## fitting the training data into the model\n",
        "XG_model.fit(X_train,Y_train)"
      ],
      "metadata": {
        "id": "FUKeLzEgRUU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred=XG_model.predict(X_test)\n",
        "Y_pred_train=XG_model.predict(X_train)\n",
        "Y_pred=XG_model.predict(X_test)"
      ],
      "metadata": {
        "id": "6LF7Tkm7gLqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## getting the training score of the model\n",
        "train_Score=XG_model.score(X_train,Y_train)\n",
        "print(f\"train_Score{XG_model.score(X_train,Y_train)}\")\n",
        "test_Score=XG_model.score(X_test,Y_test)\n",
        "print(f\"test_score{XG_model.score(X_test,Y_test)}\")\n"
      ],
      "metadata": {
        "id": "I4Fb3q7FgTdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the the actual and predicted values\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(Y_pred[:50]**2)\n",
        "plt.plot(np.array((Y_test[:50])**2))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Qd89DZAgXjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I USED XGBOOST REGRESSOR XG BOOST GIVE 94% ACCURACY."
      ],
      "metadata": {
        "id": "pXIDjwjBD1sA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "CROSS VALIDATION AND HYPERPARAMETRIC TUNING ON XGBOOST REGRESSOR "
      ],
      "metadata": {
        "id": "Nhw34i2ODjTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create the XGBRegressor object\n",
        "dreg = xgb.XGBRegressor()\n",
        "\n",
        "# Set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15],\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(\n",
        "    dreg,\n",
        "    param_grid,\n",
        "    cv=5, # 5-fold cross-validation\n",
        "    scoring='neg_mean_squared_error', # Use mean squared error as the evaluation metric\n",
        "    n_jobs=-1 # Use all available CPU cores\n",
        ")\n",
        "\n",
        "# Fit the GridSearchCV object to the data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best estimator from the GridSearchCV object\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "# Use the best estimator to make predictions on the test set\n",
        "y_predbc = best_estimator.predict(X_test)"
      ],
      "metadata": {
        "id": "840b5qnVhfs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_score= 1 * best_estimator.score(X_train, Y_train)\n",
        "test_score = 1 * best_estimator.score(X_test, Y_test)\n",
        "print('Training score: {:.2f}'.format(training_score))\n",
        "print('Testing score: {:.2f}'.format(test_score))"
      ],
      "metadata": {
        "id": "AbuRKwBGkk-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the the actual and predicted values\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(y_predbc[:50]**2)\n",
        "plt.plot(np.array((Y_test[:50])**2))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0-KvW6nAivAN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "b_Hwdb52EyFr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter tuning helps to find the optimal combination of hyperparameters, which can improve the accuracy of the model. This is especially important when dealing with complex datasets.\n",
        " XGBoost is already a fast algorithm, but hyperparameter tuning can help to further optimize the training process, resulting in faster training times.\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YES , BEFORE USING HYPERPARAMETRIC TUNING TESTING SCORE WAS 94 BUT NOW TESTING SCORE IS 97."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RANDOM FOREST**"
      ],
      "metadata": {
        "id": "JJYEFLYkMiMn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the random forest regressor model\n",
        "rf_model = RandomForestRegressor()\n",
        "\n",
        "# Fit the model to the training data\n",
        "rf_model.fit(X_train, Y_train)\n",
        "# Make predictions on the test data\n",
        "y_pred_nrg = rf_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using mean squared error\n",
        "mse = mean_squared_error(Y_test, y_pred_nrg)\n",
        "print(\"Mean squared error: \", mse)\n",
        "## getting the training score of the model\n",
        "train_Score=rf_model.score(X_train,Y_train)\n",
        "print(f\"train_Score{rf_model.score(X_train,Y_train)}\")\n",
        "test_Score=rf_model.score(X_test,Y_test)\n",
        "print(f\"test_score{rf_model.score(X_test,Y_test)}\")"
      ],
      "metadata": {
        "id": "5Szu7QsvMin7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the the actual and predicted values\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(y_pred_nrg[:50]**2)\n",
        "plt.plot(np.array((Y_test[:50])**2))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "e3j3nB_uMu6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RANDOM FOREST GIVE A TRAINING SCORE 99% AND TESTING SCORE 99%.**"
      ],
      "metadata": {
        "id": "P_HS6CJxNYvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation with hyperparameter optimization techniques GridSearch**"
      ],
      "metadata": {
        "id": "NRGntrmmNOAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Creating the RandomForestRegressor object\n",
        "rdf = RandomForestRegressor()\n",
        "\n",
        "# Set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'n_estimators': [25, 50, 100],\n",
        "    'max_depth': [5, 10, 15],\n",
        "}\n",
        "\n",
        "# Creating the GridSearchCV object\n",
        "grid_search = GridSearchCV(\n",
        "    rdf,\n",
        "    param_grid,\n",
        "    cv=5, # 5-fold cross-validation\n",
        "    scoring='neg_mean_squared_error', # Use mean squared error as the evaluation metric\n",
        "    n_jobs=-1 # Use all available CPU cores\n",
        ")\n",
        "\n",
        "# Fiting the GridSearchCV object to the data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Get the best estimator from the GridSearchCV object\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "# Use the best estimator to make predictions on the test set\n",
        "y_predcv = best_estimator.predict(X_test)\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_score = 1 * best_estimator.score(X_train, Y_train)\n",
        "test_score = 1 * best_estimator.score(X_test, Y_test)\n",
        "print('Train score: {:.2f}'.format(train_score))\n",
        "print('Test score: {:.2f}'.format(test_score))"
      ],
      "metadata": {
        "id": "wXSyrLM7oFUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plotting the the actual and predicted values\n",
        "plt.figure(figsize=(10,7))\n",
        "plt.plot(y_predcv[:50]**2)\n",
        "plt.plot(np.array((y_predcv[:50])**2))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kPsdCOkXnzSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DECISION TREE**"
      ],
      "metadata": {
        "id": "bp-eF_lJolcX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "tree = DecisionTreeRegressor(min_samples_leaf=10)\n",
        "treereg =tree.fit(X_train ,Y_train)\n",
        "print(\"Regression Model Score\" , \":\" ,treereg.score(X_train,Y_train),\"\\n\",\n",
        "      \"Out of sample Test score\" ,\":\",treereg.score(X_test ,Y_test))\n",
        "print(\"\\n\")\n",
        "yk_predicted = treereg.predict(X_train)\n",
        "yk_test_predicted = treereg.predict(X_test)"
      ],
      "metadata": {
        "id": "btFrre45o4x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#constructing a prediction dataframe with the actual and predicted sales values.\n",
        "df_prediction = pd.DataFrame(np.array((Y_test)**2) ,columns =[\"Y_test\"])\n",
        "df_prediction[\"Y_test_predicted\"] = np.array((yk_test_predicted)**2)\n",
        "df_prediction.head(25)"
      ],
      "metadata": {
        "id": "yZbXsCFspFYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the DecisionTreeRegressor object\n",
        "tree = DecisionTreeRegressor()\n",
        "\n",
        "# Set up the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'min_samples_leaf': [5, 10, 20, 50]\n",
        "}\n",
        "\n",
        "# Create the GridSearchCV object\n",
        "grid_search = GridSearchCV(\n",
        "    tree,\n",
        "    param_grid,\n",
        "    cv=5, # 5-fold cross-validation\n",
        "    scoring='neg_mean_squared_error', # Use mean squared error as the evaluation metric\n",
        "    n_jobs=-1 # Use all available CPU cores\n",
        ")\n",
        "\n",
        "# Fiting the GridSearchCV object to the data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "\n",
        "# Geting the best estimator from the GridSearchCV object\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "# Use the best estimator to make predictions on the test set\n",
        "y_pred = best_estimator.predict(X_test)\n",
        "\n",
        "# Geting the train score and test score\n",
        "train_score = 1 * best_estimator.score(X_train, Y_train)\n",
        "test_score = 1 * best_estimator.score(X_test, Y_test)\n",
        "\n",
        "# Print out the train score and test score\n",
        "print('Train score: {:.2f}'.format(train_score))\n",
        "print('Test score: {:.2f}'.format(test_score))"
      ],
      "metadata": {
        "id": "WQL4KaIXr0Oh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#constructing a prediction dataframe with the actual and predicted sales values.\n",
        "df_prediction = pd.DataFrame(np.array((Y_test)**2) ,columns =[\"Y_test\"])\n",
        "df_prediction[\"Y_test_predicted\"] = np.array((yk_test_predicted)**2)\n",
        "df_prediction.head(25)"
      ],
      "metadata": {
        "id": "YRDHFYtSsEcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xg boost with hyperparametic tuning  GIVE US 100% Training accuracy and 99% testing score and random forest giving 99% traing score and 99% testing score.\n",
        "both model give equal score.xg boost with hyperparametric tuning is not taking time to execute and train data but random forest taking some time. so i will xg  boost with hyperparametric tuning."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conclusion:-** With 99% Training Accuracy and 99% Testing Accuracy, Random Forest and Xgoost has proven to be the most efficient model out of the algorithms used in our model, including Linear Regression, Lasso Regression, Ridge Regression, Decision Tree, and Random Forest. Whereas, Linear Regression, Lasso and Rigde are not fitting well into the data points."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}